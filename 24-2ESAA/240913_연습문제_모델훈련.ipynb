{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggoddll99/ds_study/blob/main/240913_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C_%EB%AA%A8%EB%8D%B8%ED%9B%88%EB%A0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **모델 훈련 연습 문제**\n",
        "___\n",
        "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
        "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
        "___\n"
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치 경사 하강법, 확률적 경사 하강법, 미니배치 경사 하강법"
      ],
      "metadata": {
        "id": "jplIOX98IkVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
        "___"
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습률이 너무 높다. 학습률을 낮춰야 한다."
      ],
      "metadata": {
        "id": "S6T48jXgMhJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
        "___"
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "과소적합의 문제이므로 높은 편향이 문제이다.\n",
        "높은 편향 : 일반화 오차 중에서 편향은 잘못된 가정으로 인한 것이다. 예를 들어 데이터가 실제로는 2차인데 선형으로 가정하는 경우이다. 편향이 큰 모델은 훈련 데이터에 과소적합되기 쉽다. 규제 하이퍼파라미터 $\\alpha$는 줄여야 한다. ($\\alpha$를 증가시킬수록 모델의 분산은 줄지만 편향은 커지므로)"
      ],
      "metadata": {
        "id": "Agw2GXKQM_D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
        "___\n",
        "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
        "- 릿지 회귀 대신 라쏘 회귀\n",
        "- 라쏘 회귀 대신 엘라스틱넷"
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 규제가 있는 모델이 없는 모델보다 성능이 좋다.\n",
        "* 라쏘 모델은 덜 중요한 특성의 가중치를 0으로 만들기 때문에 모델을 단순화할 수 있다.\n",
        "* 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 보통 라쏘가 문제를 일으키므로"
      ],
      "metadata": {
        "id": "vH2gq_1gPWSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **추가) 조기 종료를 사용한 배치 경사 하강법으로 iris 데이터를 활용해 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QIZpOEYJVIAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "8pXDQ_fU8Nz0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 2. 데이터 분할 설정\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "# 3. 데이터를 섞어서 훈련, 검증, 테스트 세트로 나눔\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "\n",
        "X_valid = X[rnd_indices[train_size:train_size + validation_size]]\n",
        "y_valid = y[rnd_indices[train_size:train_size + validation_size]]\n",
        "\n",
        "X_test = X[rnd_indices[train_size + validation_size:]]\n",
        "y_test = y[rnd_indices[train_size + validation_size:]]\n",
        "\n",
        "# 4. 원-핫 인코딩 함수 정의\n",
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1  # 클래스의 총 개수를 계산\n",
        "    m = len(y)\n",
        "    Y_one_hot = np.zeros((m, n_classes))\n",
        "\n",
        "    # 각 데이터에 맞는 클래스 위치에 1을 할당\n",
        "    Y_one_hot[np.arange(m), y] = 1\n",
        "    return Y_one_hot\n",
        "\n",
        "# 5. 타겟 레이블을 원-핫 인코딩으로 변환\n",
        "y_train_one_hot = to_one_hot(y_train)\n",
        "y_valid_one_hot = to_one_hot(y_valid)\n",
        "y_test_one_hot = to_one_hot(y_test)\n",
        "\n",
        "# 데이터의 크기 확인 (선택적 출력)\n",
        "print(\"훈련 데이터 크기:\", X_train.shape, y_train_one_hot.shape)\n",
        "print(\"검증 데이터 크기:\", X_valid.shape, y_valid_one_hot.shape)\n",
        "print(\"테스트 데이터 크기:\", X_test.shape, y_test_one_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MQGkmiQRwsk",
        "outputId": "48e9f89b-7b50-4388-8d7c-7fb3128f769d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터 크기: (90, 4) (90, 3)\n",
            "검증 데이터 크기: (30, 4) (30, 3)\n",
            "테스트 데이터 크기: (30, 4) (30, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 소프트맥스 함수\n",
        "def softmax(logits):\n",
        "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # 수치 안정성을 위한 최대값 뺄셈\n",
        "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "# 4. 크로스 엔트로피 손실 함수\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))  # 작은 값을 더해 수치적으로 안정성 확보\n",
        "\n",
        "    # 5. 소프트맥스 회귀 학습 함수 (경사 하강법)\n",
        "def train_softmax_regression(X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=5000, tolerance=1e-4, patience=10):\n",
        "    n_samples, n_features = X_train.shape  # 훈련 데이터의 샘플 수와 특징 수\n",
        "    n_classes = y_train.shape[1]  # 클래스 수 (원-핫 인코딩된 y_train의 두 번째 차원)\n",
        "\n",
        "    # 가중치 및 편향 초기화\n",
        "    weights = np.random.randn(n_features, n_classes)\n",
        "    biases = np.zeros((1, n_classes))\n",
        "\n",
        "    best_val_loss = np.inf  # 검증 손실 최솟값 초기화\n",
        "    patience_counter = 0  # 조기 종료를 위한 patience 카운터\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 1. 로짓 계산: WX + b\n",
        "        logits = np.dot(X_train, weights) + biases\n",
        "        y_pred = softmax(logits)\n",
        "\n",
        "        # 2. 손실 함수 계산\n",
        "        loss = cross_entropy_loss(y_train, y_pred)\n",
        "\n",
        "        # 3. 경사 계산\n",
        "        grad_logits = (y_pred - y_train) / n_samples  # y_pred와 y_train의 차이를 평균으로 나눔\n",
        "        grad_weights = np.dot(X_train.T, grad_logits)  # 가중치에 대한 경사 계산\n",
        "        grad_biases = np.sum(grad_logits, axis=0, keepdims=True)  # 편향에 대한 경사 계산\n",
        "\n",
        "        # 4. 가중치 및 편향 업데이트\n",
        "        weights -= learning_rate * grad_weights\n",
        "        biases -= learning_rate * grad_biases\n",
        "\n",
        "        # 5. 검증 데이터에 대한 손실 계산\n",
        "        val_logits = np.dot(X_val, weights) + biases\n",
        "        y_val_pred = softmax(val_logits)\n",
        "        val_loss = cross_entropy_loss(y_val, y_val_pred)\n",
        "\n",
        "        # 6. 조기 종료 조건 확인\n",
        "        if val_loss < best_val_loss - tolerance:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0  # 검증 손실이 개선되면 patience_counter 초기화\n",
        "        else:\n",
        "            patience_counter += 1  # 개선되지 않으면 patience_counter 증가\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"조기 종료: {epoch} 에포크에서 종료됨. 검증 손실: {val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "        # 7. 매 100 에포크마다 학습 상태 출력\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"에포크 {epoch}: 훈련 손실 = {loss:.4f}, 검증 손실 = {val_loss:.4f}\")\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "# 6. 모델 학습\n",
        "weights, biases = train_softmax_regression(X_train, y_train_one_hot, X_valid, y_valid_one_hot)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"훈련 완료 후 가중치:\", weights)\n",
        "print(\"훈련 완료 후 편향:\", biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aic2wGgXXTA",
        "outputId": "fbacf6e1-c79c-4998-f861-6675e86f8618"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에포크 0: 훈련 손실 = 3.2849, 검증 손실 = 3.2774\n",
            "에포크 100: 훈련 손실 = 0.7125, 검증 손실 = 0.8067\n",
            "에포크 200: 훈련 손실 = 0.6141, 검증 손실 = 0.7132\n",
            "에포크 300: 훈련 손실 = 0.5520, 검증 손실 = 0.6528\n",
            "에포크 400: 훈련 손실 = 0.5075, 검증 손실 = 0.6086\n",
            "에포크 500: 훈련 손실 = 0.4729, 검증 손실 = 0.5736\n",
            "에포크 600: 훈련 손실 = 0.4445, 검증 손실 = 0.5444\n",
            "에포크 700: 훈련 손실 = 0.4203, 검증 손실 = 0.5192\n",
            "에포크 800: 훈련 손실 = 0.3993, 검증 손실 = 0.4970\n",
            "에포크 900: 훈련 손실 = 0.3807, 검증 손실 = 0.4771\n",
            "에포크 1000: 훈련 손실 = 0.3640, 검증 손실 = 0.4592\n",
            "에포크 1100: 훈련 손실 = 0.3490, 검증 손실 = 0.4428\n",
            "에포크 1200: 훈련 손실 = 0.3353, 검증 손실 = 0.4279\n",
            "에포크 1300: 훈련 손실 = 0.3228, 검증 손실 = 0.4141\n",
            "에포크 1400: 훈련 손실 = 0.3114, 검증 손실 = 0.4014\n",
            "에포크 1500: 훈련 손실 = 0.3008, 검증 손실 = 0.3897\n",
            "에포크 1600: 훈련 손실 = 0.2911, 검증 손실 = 0.3787\n",
            "에포크 1700: 훈련 손실 = 0.2821, 검증 손실 = 0.3686\n",
            "에포크 1800: 훈련 손실 = 0.2737, 검증 손실 = 0.3591\n",
            "에포크 1900: 훈련 손실 = 0.2659, 검증 손실 = 0.3503\n",
            "에포크 2000: 훈련 손실 = 0.2586, 검증 손실 = 0.3420\n",
            "에포크 2100: 훈련 손실 = 0.2518, 검증 손실 = 0.3342\n",
            "에포크 2200: 훈련 손실 = 0.2454, 검증 손실 = 0.3269\n",
            "에포크 2300: 훈련 손실 = 0.2395, 검증 손실 = 0.3200\n",
            "에포크 2400: 훈련 손실 = 0.2338, 검증 손실 = 0.3135\n",
            "에포크 2500: 훈련 손실 = 0.2285, 검증 손실 = 0.3074\n",
            "에포크 2600: 훈련 손실 = 0.2235, 검증 손실 = 0.3016\n",
            "에포크 2700: 훈련 손실 = 0.2187, 검증 손실 = 0.2961\n",
            "에포크 2800: 훈련 손실 = 0.2142, 검증 손실 = 0.2909\n",
            "에포크 2900: 훈련 손실 = 0.2100, 검증 손실 = 0.2860\n",
            "에포크 3000: 훈련 손실 = 0.2059, 검증 손실 = 0.2813\n",
            "에포크 3100: 훈련 손실 = 0.2021, 검증 손실 = 0.2768\n",
            "에포크 3200: 훈련 손실 = 0.1984, 검증 손실 = 0.2725\n",
            "에포크 3300: 훈련 손실 = 0.1949, 검증 손실 = 0.2685\n",
            "에포크 3400: 훈련 손실 = 0.1916, 검증 손실 = 0.2646\n",
            "에포크 3500: 훈련 손실 = 0.1884, 검증 손실 = 0.2609\n",
            "에포크 3600: 훈련 손실 = 0.1854, 검증 손실 = 0.2574\n",
            "에포크 3700: 훈련 손실 = 0.1825, 검증 손실 = 0.2540\n",
            "에포크 3800: 훈련 손실 = 0.1797, 검증 손실 = 0.2507\n",
            "에포크 3900: 훈련 손실 = 0.1770, 검증 손실 = 0.2476\n",
            "에포크 4000: 훈련 손실 = 0.1744, 검증 손실 = 0.2446\n",
            "에포크 4100: 훈련 손실 = 0.1719, 검증 손실 = 0.2418\n",
            "에포크 4200: 훈련 손실 = 0.1695, 검증 손실 = 0.2390\n",
            "에포크 4300: 훈련 손실 = 0.1672, 검증 손실 = 0.2363\n",
            "에포크 4400: 훈련 손실 = 0.1650, 검증 손실 = 0.2338\n",
            "에포크 4500: 훈련 손실 = 0.1629, 검증 손실 = 0.2313\n",
            "에포크 4600: 훈련 손실 = 0.1609, 검증 손실 = 0.2290\n",
            "에포크 4700: 훈련 손실 = 0.1589, 검증 손실 = 0.2267\n",
            "에포크 4800: 훈련 손실 = 0.1570, 검증 손실 = 0.2245\n",
            "에포크 4900: 훈련 손실 = 0.1551, 검증 손실 = 0.2223\n",
            "훈련 완료 후 가중치: [[ 1.22526308 -0.24238048 -1.07888668]\n",
            " [ 0.19868068 -0.15007786 -2.34181817]\n",
            " [-1.51199945  1.52572923  3.04744232]\n",
            " [-2.71734571 -2.37530431  0.96379837]]\n",
            "훈련 완료 후 편향: [[ 0.21643896  0.54690703 -0.76334599]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 예측 함수\n",
        "def predict(X, weights, biases):\n",
        "    logits = np.dot(X, weights) + biases\n",
        "    y_pred = softmax(logits)\n",
        "    return np.argmax(y_pred, axis=1)  # 가장 높은 확률을 가진 클래스 반환\n",
        "\n",
        "# 8. 정확도 계산 함수\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# 9. 테스트 세트에서 평가\n",
        "y_test_pred = predict(X_test, weights, biases)\n",
        "test_accuracy = accuracy(y_test, y_test_pred)\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"테스트 세트 정확도: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i4vBgbCZQjX",
        "outputId": "d28769e8-3824-4e2a-d818-9c769fc264fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 세트 정확도: 0.9667\n"
          ]
        }
      ]
    }
  ]
}
